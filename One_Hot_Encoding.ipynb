{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt') # Download 'punkt' from nltk if it's not downloaded\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO_eI7mugOOE",
        "outputId": "c729fda6-5687-40cf-dcbf-8f1007dfc5a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Text = \"\"\"Geeks For Geeks.\n",
        "\t\tGeeks Learning Together.\n",
        "\t\tGeeks For Geeks is famous for DSA.\n",
        "\t\tLearning DSA\"\"\"\n",
        "sentences = sent_tokenize(Text)\n",
        "sentences = [sent.lower().replace(\".\", \"\") for sent in sentences]\n",
        "print('Tokenized Sentences :', sentences)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8Pupu3SgfO3",
        "outputId": "450672d0-1ffc-470b-e308-ce78b9d5af7f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentences : ['geeks for geeks', 'geeks learning together', 'geeks for geeks is famous for dsa', 'learning dsa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the vocabulary\n",
        "vocab = {}\n",
        "count = 0\n",
        "for sent in sentences:\n",
        "\tfor word in sent.split():\n",
        "\t\tif word not in vocab:\n",
        "\t\t\tcount = count + 1\n",
        "\t\t\tvocab[word] = count\n",
        "print('vocabulary :', vocab)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuWoR7MAgvvW",
        "outputId": "fc72baf3-3078-4daa-e9dd-76d703f2dc15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary : {'geeks': 1, 'for': 2, 'learning': 3, 'together': 4, 'is': 5, 'famous': 6, 'dsa': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One Hot Encoding\n",
        "def OneHotEncoder(text):\n",
        "\tonehot_encoded = []\n",
        "\tfor word in text.split():\n",
        "\t\ttemp = [0]*len(vocab)\n",
        "\t\tif word in vocab:\n",
        "\t\t\ttemp[vocab[word]-1] = 1\n",
        "\t\t\tonehot_encoded.append(temp)\n",
        "\treturn onehot_encoded\n"
      ],
      "metadata": {
        "id": "b9dkuiqMgxyw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print('\\n',sentences[0])\n",
        "print('OneHotEncoded vector for sentence : \"',\n",
        "\tsentences[0], '\"is \\n', OneHotEncoder(sentences[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHUY1uzjgz8X",
        "outputId": "1349d576-4ba5-4e2c-af2e-e1e61a218dc6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OneHotEncoded vector for sentence : \" geeks for geeks \"is \n",
            " [[1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Interpretation:\n",
        "\n",
        "The code tokenizes the given text into sentences, creates a vocabulary of unique words, and performs one-hot encoding on the first sentence.\n",
        "\n",
        "One-hot encoding represents each word as a binary vector where all elements are zero except for the one corresponding to the word's index in the vocabulary.\n",
        "\n",
        "This allows for numerical representation of text data, which can be used as input for machine learning models.\n"
      ],
      "metadata": {
        "id": "xA7r24kBg3PY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9yqD8YfDg2dJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}